# ğŸ‘§ğŸ¼ HELLA â€“ Your Personal AI Assistant

HELLA is a **real-time multimodal AI agent** that listens, thinks, looks, and responds like a human.  
It combines **LLMs, speech, and vision** to create a seamless AI assistant experience.

---

## **ğŸš€ Features**

| Function | Tech Used |
|-----------|-----------|
| **Talk to You (Speech-to-Text)** | **Whisper Large V3 (Groq API)** |
| **Talk Back (Text-to-Speech)** | **ElevenLabs API** |
| **Answer Questions & Reasoning** | **Gemini-2.0-Flash (Google Generative AI)** |
| **See Through Camera** | **LLaMA-4 Maverick Vision Model (Groq Vision API)** |
| **Real-Time UI** | **Gradio Interface** |
| **Agentic Decision Making** | **LangGraph ReAct Agent** |

---

## **ğŸ§  How It Works**

1. **Continuous Listening**  
   HELLA listens to your voice input using **Whisper STT**.

2. **Agentic Reasoning**  
   Uses a **ReAct Agent (LangGraph)** with **Gemini-2.0-Flash** to process the query.

3. **Vision Capabilities**  
   If the query needs visual input (like "Do I have my phone in my hand?"), HELLA:
   - Captures an image via webcam  
   - Sends it to **LLaMA-4 Maverick Vision LLM** via Groq

4. **Natural Speech Output**  
   Responds with human-like audio using **ElevenLabs TTS**.

---

## **ğŸ§° Tech Stack**

- **LLMs**: Gemini-2.0-Flash, LLaMA-4 Maverick  
- **Speech-to-Text**: Whisper Large V3 via Groq API  
- **Text-to-Speech**: ElevenLabs  
- **Vision**: Groq Vision API  
- **Framework**: Gradio  
- **Environment & Package Management**: `uv` (Rust-based Python tool)

---

## **ğŸ“¦ Why `uv`?**

Instead of `pip + venv`, this project uses **`uv`** for:

- âš¡ **10x faster package installs**  
- ğŸ”’ **Deterministic builds** with lockfiles  
- ğŸš€ **Modern tooling**, no venv setup hassle  
- ğŸ¦€ **Rust-powered speed**

---

## **ğŸ“‚ Project Structure**
HELLA-AI-ASSISTANT/
â”‚
â”œâ”€â”€ ai_agent.py              # LangGraph ReAct agent setup with Gemini & Vision tools
â”œâ”€â”€ tools.py                  # Webcam capture + Vision Q&A (LLaMA-4 Maverick)
â”œâ”€â”€ speech_to_text.py         # Whisper-based STT using Groq API
â”œâ”€â”€ text_to_speech.py         # ElevenLabs API for TTS
â”œâ”€â”€ main.py                   # Gradio UI and continuous interaction loop
â”œâ”€â”€ .env                      # API keys (not shared publicly)
â”œâ”€â”€ pyproject.toml            # Project config
â”œâ”€â”€ uv.lock                   # Lockfile for reproducibility

---

## **ğŸ”‘ API Keys (Add to `.env` file)**
GROQ_API_KEY=your_groq_api_key
GOOGLE_API_KEY=your_google_api_key
ELEVENLABS_API_KEY=your_elevenlabs_api_key


---

## **ğŸ–¥ï¸ Run Locally**

1ï¸âƒ£ Install [uv](https://github.com/astral-sh/uv)

```bash
curl -Ls https://astral.sh/uv/install.sh | sh

python main.py


ğŸ“¸ Example Use Cases
	â€¢	Ask general knowledge questions
	â€¢	Ask visual queries (e.g., â€œDo I have a phone in my hand?â€)
	â€¢	Smart home conversational agent
	â€¢	Multimodal AI demos




---

### **Next Step:**

- Copy this into your repoâ€™s `README.md`  
- Commit & push:

```bash
git add README.md
git commit -m "Added professional README"
git push
